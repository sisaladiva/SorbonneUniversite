{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "749ddd0e-47c6-42d8-b7d1-cb2561c985c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import poisson\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as dist\n",
    "from scipy.stats import ncx2\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55fc41d9-0759-4cfa-a4f0-0a38d0fc1c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utils:\n",
    "    # Convert string parameter into torch activation function\n",
    "    @staticmethod\n",
    "    def get_activation_function(activation_function_str, negative_slope):\n",
    "        if activation_function_str == \"Lrelu\":\n",
    "            activation = torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation_function_str == \"relu\":\n",
    "            activation = torch.nn.ReLU()\n",
    "        elif activation_function_str == \"sigmoid\":\n",
    "            activation = torch.nn.Sigmoid()\n",
    "        else:\n",
    "            raise ValueError(\"Activation function not recognized : {}\".format(activation_function_str))\n",
    "        return activation\n",
    "    \n",
    "    # Convert string parameter into torch final activation function\n",
    "    @staticmethod\n",
    "    def get_final_activation_function(final_activation):\n",
    "        if final_activation == \"Sigmoid\":\n",
    "            final_activation = lambda x: torch.sigmoid(x)\n",
    "        elif final_activation == \"Clamping\":\n",
    "            final_activation = lambda x: torch.clamp(x, min=0., max=1.)\n",
    "        elif final_activation == \"Linear\":\n",
    "            final_activation = lambda x: x\n",
    "        else:\n",
    "            raise ValueError(\"final_activation must be Sigmoid, Clamping or Linear\")\n",
    "        return final_activation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4626bf5-d229-4850-a93e-41d2c9ba4881",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conditional_Chi_Generator(nn.Module):\n",
    "    def __init__(self,\n",
    "                 noise_size: int=1,\n",
    "                 min_d: float=1.,\n",
    "                 max_d: float=1.,\n",
    "                 min_lda: float=1.,\n",
    "                 max_lda: float=1.,\n",
    "                 hidden_dim: int=100,\n",
    "                 activation_function: str=\"Lrelu\",\n",
    "                 final_activation: str=\"Sigmoid\",\n",
    "                 label_size: int=10,\n",
    "                 negative_slope: float=0.1,\n",
    "                 nb_hidden_layers_G: int=1,\n",
    "                 eps: float=0.,\n",
    "                 **kwargs):\n",
    "        super(Conditional_Chi_Generator, self).__init__()\n",
    "        self.min_d = min_d\n",
    "        self.max_d = max_d\n",
    "        self.min_lda = min_lda\n",
    "        self.max_lda = max_lda\n",
    "        self.input_dim = noise_size\n",
    "        self.nb_hidden_layers = nb_hidden_layers_G\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.activation = Utils.get_activation_function(activation_function, negative_slope)\n",
    "        self.input_noise = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "        self.label_size = label_size\n",
    "        self.input_label = nn.Linear(2, self.label_size)\n",
    "        self.fcs = nn.ModuleList()\n",
    "        self.eps = eps\n",
    "        for i in range(self.nb_hidden_layers):\n",
    "            if i == 0:\n",
    "                self.fcs.append(nn.Linear(self.hidden_dim + self.label_size, self.hidden_dim))\n",
    "            else:\n",
    "                self.fcs.append(nn.Linear(self.hidden_dim, self.hidden_dim))\n",
    "        self.output = nn.Linear(self.hidden_dim, 1)\n",
    "        self.final_activation = Utils.get_final_activation_function(final_activation)\n",
    "    \n",
    "    def forward(self, x, d, lda):\n",
    "        # Normalize d with min_d and max_d into [0, 1]\n",
    "        if self.max_d == self.min_d:\n",
    "            d = 1\n",
    "        else:\n",
    "            d = (d - self.min_d) / (self.max_d - self.min_d)\n",
    "        # Normalize lambda with min_lda and max_lda into [0, 1]\n",
    "        if self.max_lda == self.min_lda:\n",
    "            lda = 1\n",
    "        else:\n",
    "            lda = (lda - self.min_lda) / (self.max_lda - self.min_lda)        \n",
    "        return self.feed_forward(x, d, lda)\n",
    "    \n",
    "    def feed_forward(self, x, d, lda):\n",
    "        x = self.activation(self.input_noise(x))\n",
    "        params = self.activation(self.input_label(torch.cat((d, lda), 1)))\n",
    "        x = torch.cat((x, params), 1)\n",
    "        for i in range(self.nb_hidden_layers):\n",
    "            x = self.activation(self.fcs[i](x))\n",
    "        x = self.final_activation(self.output(x))\n",
    "        return x + self.eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b735760-4780-49a8-8964-50d1b1b44454",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "\n",
    "# tested_d are d values on which we will perform tests (KS divergence)\n",
    "params[\"tested_d\"] = [0.3, 0.5, 1, 10, 50, 100]\n",
    "params[\"tested_lda\"] = [0, 50, 100, 500, 1000]\n",
    "# displayed_d are d values on which we will display histograms. displayed_d has to be subset of tested_d to be displayed\n",
    "params[\"min_d\"] = 0.3\n",
    "params[\"max_d\"] = 10.\n",
    "params[\"min_lda\"] = 0.\n",
    "params[\"max_lda\"] = 100\n",
    "# alpha: chi.ppf(alpha) is network's max return value.\n",
    "# Because network return values are bounded in [0,1], 1 has to have a draw equivalent which is chi.ppf(alpha)\n",
    "params[\"alpha\"] = 0.999\n",
    "\n",
    "# Networks parameters :\n",
    "# Number of uniform draws for G\n",
    "params[\"noise_size\"] = 5\n",
    "# Hidden layer size for d inputs when parametrized (leaving it at 10 is satisfactory)\n",
    "params[\"label_size\"] = 50\n",
    "# \"Lrelu\", \"relu\" or \"sigmoid\" (default Lrelu with negative slope 0.1)\n",
    "params[\"activation_function\"] = \"Lrelu\"\n",
    "params[\"negative_slope\"] = 0.1\n",
    "# Final activation for Generator: Sigmoid, Linear or Clamping\n",
    "params[\"final_activation\"] = \"Sigmoid\"\n",
    "# hidden layer size\n",
    "params[\"hidden_dim\"] = 100\n",
    "# number of hidden layers for both D and G after input layer (default 1) : total number of hidden layers is nb_hidden_layers + 1\n",
    "params[\"nb_hidden_layers_G\"] = 2\n",
    "params[\"nb_hidden_layers_D\"] = 2\n",
    "params[\"device\"] = torch.device('cuda:0')\n",
    "params[\"M\"] = ncx2.ppf(params[\"alpha\"], params[\"max_d\"], params[\"max_lda\"])\n",
    "params[\"m\"] = ncx2.ppf(1-params[\"alpha\"], params[\"min_d\"], params[\"min_lda\"])\n",
    "\n",
    "# Load_dir_model is the folder containing the model to load\n",
    "params[\"load_dir_model\"] = \"checkpoints_CHI2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b92b9ce0-843f-47bc-a0a0-26def081ef87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "989\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m Chi2_checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_dir_model\u001b[39m\u001b[38;5;124m\"\u001b[39m], map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(Chi2_checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m Chi2_generator \u001b[38;5;241m=\u001b[39m \u001b[43mConditional_Chi_Generator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdevice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m Chi2_generator\u001b[38;5;241m.\u001b[39mload_state_dict(Chi2_checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerator_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/cuda/__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m     )\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "Chi2_checkpoint = torch.load(params[\"load_dir_model\"], map_location='cpu')\n",
    "print(Chi2_checkpoint['epoch'])\n",
    "Chi2_generator = Conditional_Chi_Generator(**params).to(params['device'])\n",
    "Chi2_generator.load_state_dict(Chi2_checkpoint['generator_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa41b7a7-66c3-44ec-9bac-48430dbc8451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CHI_generator(G, noise_size, d, _lda, size,  max_d, max_lda, M, m, alpha, device, **kwargs) :\n",
    "    if d > max_d or lda > max_lda:\n",
    "        _d = max_d\n",
    "        _lda = max_lda\n",
    "    else:\n",
    "        _d = d\n",
    "        _lda = lda\n",
    "    partial_uniforms = torch.rand(size, noise_size, device = device)\n",
    "    d_input = torch.ones(size, 1, device = device)*_d\n",
    "    lda_input = torch.ones(size, 1, device = device)*_lda\n",
    "    simulated_chi_draws = G(partial_uniforms, d_input, lda_input)*(M - m) + m\n",
    "    if d > max_d or lda > max_lda:\n",
    "        mean_1 = _d + _lda\n",
    "        var_1 = 2 * (_d + 2 * _lda)\n",
    "        mean_2 = d + lda\n",
    "        var_2 = 2 * (d + 2 * lda)\n",
    "        simulated_chi_draws = (simulated_chi_draws - mean_1) / np.sqrt(var_1) * np.sqrt(var_2) + mean_2 \n",
    "\n",
    "    return simulated_chi_draws.reshape(1,-1)[0]\n",
    "    \n",
    "def Torch_CHI_generator(d, _lda, size, device, **kwargs) :\n",
    "    if d <= 1 :\n",
    "        samples_chi2 = torch.tensor(ncx2.rvs(d, _lda, size = size), device = device)\n",
    "    else :\n",
    "        chi2_dist_GPU = dist.Chi2(torch.tensor(d-1, device = device))    \n",
    "        samples_chi2 = chi2_dist_GPU.sample(torch.tensor([size], device = device))\n",
    "        Gaussian = (torch.randn(size, device= device)+torch.sqrt(torch.tensor(_lda, device = device)))**2\n",
    "        samples_chi2 += Gaussian\n",
    "    return  samples_chi2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c47c6b6-01d6-4310-8a33-036c0954dd45",
   "metadata": {},
   "source": [
    "Plot histograms on tested_d x tested_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9bea2801-a556-45ca-ab15-1a2b90da63bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stats time = 0.2420799732208252\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstats time =\u001b[39m\u001b[38;5;124m\"\u001b[39m, time()\u001b[38;5;241m-\u001b[39mt_)\n\u001b[1;32m     11\u001b[0m t_ \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m---> 12\u001b[0m true_Chi_torch \u001b[38;5;241m=\u001b[39m \u001b[43mTorch_CHI_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_lda\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch time =\u001b[39m\u001b[38;5;124m\"\u001b[39m, time()\u001b[38;5;241m-\u001b[39m t_)\n\u001b[1;32m     14\u001b[0m t_ \u001b[38;5;241m=\u001b[39m time()\n",
      "Cell \u001b[0;32mIn[15], line 23\u001b[0m, in \u001b[0;36mTorch_CHI_generator\u001b[0;34m(d, _lda, size, device, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mTorch_CHI_generator\u001b[39m(d, _lda, size, device, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) :\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m d \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m :\n\u001b[0;32m---> 23\u001b[0m         samples_chi2 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mncx2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrvs\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_lda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m :\n\u001b[1;32m     25\u001b[0m         chi2_dist_GPU \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mChi2(torch\u001b[38;5;241m.\u001b[39mtensor(d\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, device \u001b[38;5;241m=\u001b[39m device))    \n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/cuda/__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m     )\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "size = 1000000\n",
    "\n",
    "for d in params['tested_d'] :\n",
    "    for lda in params['tested_lda'] :\n",
    "        _min = math.floor(ncx2.ppf(1 - 0.999, d, lda))\n",
    "        _max = math.ceil(ncx2.ppf(0.999, d, lda))\n",
    "        bins = np.linspace(_min, _max, 200)\n",
    "        t_ = time()\n",
    "        true_Chi_CPU = ncx2.rvs(d, lda, size = size)\n",
    "        print(\"stats time =\", time()-t_)\n",
    "        t_ = time()\n",
    "        true_Chi_torch = Torch_CHI_generator(d = d, _lda = lda, size = size, device = torch.device('cuda:0'))\n",
    "        print(\"torch time =\", time()- t_)\n",
    "        t_ = time()\n",
    "        GAN_Chi_torch = CHI_generator(G = Chi2_generator, d = d, _lda = lda, size = size, **params)\n",
    "        print(\"GAN time =\", time()-t_)\n",
    "        true_Chi = true_Chi_torch.cpu().numpy()\n",
    "        GAN_Chi = GAN_Chi_torch.detach().cpu().numpy()\n",
    "        plt.hist(true_Chi, bins=200, alpha=0.75, color = 'y', density=True, label=\"torch Chi distribution\")\n",
    "        plt.hist(GAN_Chi, bins=200, alpha=0.75, color = 'red', density=True, label=\"GAN_GPU distribution\")\n",
    "        plt.hist(true_Chi_CPU, bins=200, alpha=0.75, density=True, color = 'blue', label=\"stats chi distribution\")\n",
    "        plt.title(\"d = {}, lambda = {}\".format(str(d), str(lda)))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ead502e-82c4-4f97-a40b-a5fbde66dd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d, lda = 0.3 0\n",
      "torch cpu time = 0.17422986030578613\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch cpu time =\u001b[39m\u001b[38;5;124m\"\u001b[39m, time()\u001b[38;5;241m-\u001b[39m t_)\n\u001b[1;32m      9\u001b[0m t_ \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m---> 10\u001b[0m true_Chi_torch \u001b[38;5;241m=\u001b[39m \u001b[43mTorch_CHI_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch gpu time =\u001b[39m\u001b[38;5;124m\"\u001b[39m, time()\u001b[38;5;241m-\u001b[39m t_)\n\u001b[1;32m     12\u001b[0m t_ \u001b[38;5;241m=\u001b[39m time()\n",
      "Cell \u001b[0;32mIn[15], line 23\u001b[0m, in \u001b[0;36mTorch_CHI_generator\u001b[0;34m(d, _lda, size, device, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mTorch_CHI_generator\u001b[39m(d, _lda, size, device, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) :\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m d \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m :\n\u001b[0;32m---> 23\u001b[0m         samples_chi2 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mncx2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrvs\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_lda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m :\n\u001b[1;32m     25\u001b[0m         chi2_dist_GPU \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mChi2(torch\u001b[38;5;241m.\u001b[39mtensor(d\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, device \u001b[38;5;241m=\u001b[39m device))    \n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/cuda/__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m     )\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "size = 1000000\n",
    "\n",
    "for d in params['tested_d'] :\n",
    "    for lda in params['tested_lda'] :\n",
    "        print(\"d, lda =\", d, lda)\n",
    "        t_ = time()\n",
    "        true_Chi_torch = Torch_CHI_generator(d, lda, size, device = torch.device('cpu'))\n",
    "        print(\"torch cpu time =\", time()- t_)\n",
    "        t_ = time()\n",
    "        true_Chi_torch = Torch_CHI_generator(d, lda, size, device = torch.device('cuda:0'))\n",
    "        print(\"torch gpu time =\", time()- t_)\n",
    "        t_ = time()\n",
    "        GAN_Chi_torch = CHI_generator(Chi2_generator, d, lda, size, **params)\n",
    "        print(\"GAN time =\", time()-t_, \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f7e9baa-0575-4d0e-bf72-b461b48af7de",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m chi2_dist_GPU \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mChi2(torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mdf\u001b[49m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(chi2_dist_GPU)\n\u001b[1;32m      4\u001b[0m t_ \u001b[38;5;241m=\u001b[39m time()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "chi2_dist_GPU = dist.Chi2(torch.tensor(df-1, device = torch.device('cuda:0')))\n",
    "print(chi2_dist_GPU)\n",
    "\n",
    "t_ = time()\n",
    "samples_chi2 = chi2_dist_GPU.sample(torch.tensor([1000000], device = torch.device('cuda:0')))\n",
    "print(time()-t_)\n",
    "t_ = time()\n",
    "samples_chi2 = chi2_dist.sample([1000000])\n",
    "print(time()-t_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad446f75-cf19-4162-97bb-8796ca05b2df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91c5580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10a668a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb82553",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
